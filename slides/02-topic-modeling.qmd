---
title: "Text Mining"
subtitle: "USING TIDY DATA PRINCIPLES"
author: "Julia Silge"
format:
  revealjs: 
    footer: <https://juliasilge.github.io/asepelt-2024/>
    theme: [default, custom.scss]
    preview-links: auto
    incremental: true
    width: 1280
    height: 720
    title-slide-attributes: 
      data-background-image: images/don-quixote.png
      data-background-size: contain
      data-background-opacity: "0.2"
knitr:
  opts_chunk: 
    echo: true
    collapse: true
    comment: "#>"    
---

```{r}
#| include: false
#| file: setup.R
```

# Hello!

<center>

<img src="https://github.com/juliasilge.png" style="border-radius: 50%;" width="300px"/>

[{{< fa brands github >}} \@juliasilge](https://github.com/juliasilge)

[{{< fa brands mastodon >}} \@juliasilge\@fosstodon.org](https://fosstodon.org/@juliasilge)

[{{< fa brands youtube >}} youtube.com/juliasilge](https://www.youtube.com/juliasilge)

[{{< fa link >}} juliasilge.com](https://juliasilge.com/)

</center>

## Let's install some packages

```{r}
#| eval: false
install.packages(c("tidyverse", 
                   "tidytext",
                   "stopwords",
                   "gutenbergr",
                   "stm"))
```

## Workflow for text mining/modeling {background-image="images/tmwr_0601.png" background-size="60%" background-color="white"}

::: footer
:::

# Topic modeling {background-image="images/don-quixote.png" background-size="contain" background-opacity="0.2"}

. . .

üìñ Each DOCUMENT = mixture of topics

. . .

üìë Each TOPIC = mixture of tokens

. . .

::: {.callout-tip}
Topic modeling is an example of _unsupervised_ machine learning.
:::

##  {background-image="images/top_tags-1.png" background-size="60%" background-color="white"}

::: footer
:::

# GREAT LIBRARY HEIST üïµ {background-image="images/don-quixote.png" background-size="contain" background-opacity="0.2"}

## Download your text data

```{r}
library(tidyverse)
library(gutenbergr)

books <- gutenberg_download(c(36, 55, 158, 768),
                            meta_fields = "title",
                            mirror = my_mirror)
books |>
    count(title)
```

## Someone has torn up your books! üò≠

What do you predict will happen if we run the following code? ü§î

```{r}
#| eval: false
books_by_document <- books |>
    group_by(title) |>
    mutate(document = row_number() %/% 500) |>
    ungroup() |>
    unite(document, title, document)

glimpse(books_by_document)
```

## Someone has torn up your books! üò≠

What do you predict will happen if we run the following code? ü§î

```{r}
books_by_document <- books |>
    group_by(title) |>
    mutate(document = row_number() %/% 500) |>
    ungroup() |>
    unite(document, title, document)

glimpse(books_by_document)
```

## Can we put them back together?

```{r}
#| code-line-numbers: "|4"
library(tidytext)

word_counts <- books_by_document |>
    unnest_tokens(word, text) |> 
    anti_join(get_stopwords(source = "smart")) |>
    count(document, word, sort = TRUE)

glimpse(word_counts)
```

. . .

::: {.callout-tip}
The dataset `word_counts` contains the counts of words per line.
:::

## Can we put them back together?

```{r}
#| code-line-numbers: "|2|4"
words_sparse <- word_counts |>
    cast_sparse(document, word, n)

class(words_sparse)
dim(words_sparse)
```

. . .

::: {.callout-tip}
Is `words_sparse` a tidy dataset?
:::

## Train a topic model

Use a sparse matrix or a `quanteda::dfm` object as input:

```{r}
library(stm)
topic_model <- stm(words_sparse, K = 4, 
                   verbose = FALSE, 
                   init.type = "Spectral")
```

## Train a topic model

Use a sparse matrix or a `quanteda::dfm` object as input:

```{r}
summary(topic_model)
```

## Explore the topic model output

```{r}
chapter_topics <- tidy(topic_model, matrix = "beta")
chapter_topics
```

## Explore the topic model output

[U N S C R A M B L E]{.scramble}

```
top_terms <- chapter_topics |>

ungroup() |>

group_by(topic) |>

arrange(topic, -beta)

slice_max(beta, n = 10) |>
```

## Explore the topic model output

```{r}
top_terms <- chapter_topics |>
    group_by(topic) |>
    slice_max(beta, n = 10) |>
    ungroup() |>
    arrange(topic, -beta)
```

## Explore the topic model output

```{r}
top_terms
```

## Explore the topic model output

```{r}
#| eval: false
top_terms |>
    mutate(term = fct_reorder(term, beta)) |>
    ggplot(aes(beta, term, fill = factor(topic))) + 
    geom_col(show.legend = FALSE) +
    facet_wrap(vars(topic), scales = "free")
```

## 

```{r}
#| echo: false
#| fig-align: center
#| fig-width: 7
#| fig-height: 6
top_terms |>
    ggplot(aes(beta, reorder_within(term, beta, topic), fill = factor(topic))) +
    geom_col(show.legend = FALSE) +
    facet_wrap(vars(topic), scales = "free") +
    scale_x_continuous(expand = c(0,0)) +
    scale_y_reordered() +
    labs(x = expression(beta), y = NULL)
```

::: footer
:::

# Identify important words {background-image="images/don-quixote.png" background-size="contain" background-opacity="0.2"}

. . .

‚≠ê FREX

‚¨ÜÔ∏è LIFT

## High FREX words

High frequency *and* high exclusivity

```{r}
tidy(topic_model, matrix = "frex")
```

## High lift words

Topic-word distribution **divided** by word count distribution

```{r}
tidy(topic_model, matrix = "lift")
```

#

<center>

{{< video https://www.youtube.com/embed/2wcDYVb-2AY width="960" height="540" >}}

</center>

## How are documents classified?

```{r}
chapters_gamma <- tidy(topic_model, matrix = "gamma",
                       document_names = rownames(words_sparse))
chapters_gamma
```

## How are documents classified?

What do you predict will happen if we run the following code? ü§î

```{r}
#| eval: false
chapters_parsed <- chapters_gamma |>
    separate(document, c("title", "chapter"), 
             sep = "_", convert = TRUE)

glimpse(chapters_parsed)
```

## How are documents classified?

What do you predict will happen if we run the following code? ü§î

```{r}
chapters_parsed <- chapters_gamma |>
    separate(document, c("title", "chapter"), 
             sep = "_", convert = TRUE)

glimpse(chapters_parsed)
```

## How are documents classified?

[U N S C R A M B L E]{.scramble}

```
chapters_parsed |>

ggplot(aes(factor(topic), gamma)) +

facet_wrap(vars(title))

mutate(title = fct_reorder(title, gamma * topic)) |>

geom_boxplot() +
```

## How are documents classified?

```{r}
#| eval: false
chapters_parsed |>
    mutate(title = fct_reorder(title, gamma * topic)) |>
    ggplot(aes(factor(topic), gamma)) +
    geom_boxplot() +
    facet_wrap(vars(title))
```

## 

```{r}
#| echo: false
#| fig-align: center
#| fig-width: 8
#| fig-height: 6
chapters_parsed |>
    mutate(title = fct_reorder(title, gamma * topic)) |>
    ggplot(aes(factor(topic), gamma, color = factor(topic))) +
    geom_boxplot(show.legend = FALSE) +
    facet_wrap(vars(title)) +
    labs(x = "Topic", y = expression(gamma)) +
    theme_light_plex()
```

::: footer
:::

# GOING FARTHER üöÄ {background-image="images/don-quixote.png" background-size="contain" background-opacity="0.2"}

## Tidying model output

Which words in each document are assigned to which topics?

::: {.nonincremental}
-   `augment()`
-   Add information to each observation in the original data
:::

## Using stm

::: {.nonincremental}
-   Document-level covariates

```{r, eval=FALSE}
topic_model <- stm(words_sparse, 
                   K = 0, init.type = "Spectral",
                   prevalence = ~s(Year),
                   data = covariates,
                   verbose = FALSE)
```

-   How do we choose $K$?üòï

-   Use functions for `semanticCoherence()`, `checkResiduals()`,
    `exclusivity()`, and more!

-   Check out <http://www.structuraltopicmodel.com/>

:::

# 

<center>

{{< video https://www.youtube.com/embed/rXDv0ZuX0Fc width="960" height="540" >}}

</center>

## Workflow for text mining/modeling {background-image="images/tmwr_0601.png" background-size="60%" background-color="white"}

::: footer
:::

# Thanks!

<center>

<img src="https://github.com/juliasilge.png" style="border-radius: 50%;" width="300px"/>

[{{< fa brands github >}} \@juliasilge](https://github.com/juliasilge)

[{{< fa brands mastodon >}} \@juliasilge\@fosstodon.org](https://fosstodon.org/@juliasilge)

[{{< fa brands youtube >}} youtube.com/juliasilge](https://www.youtube.com/juliasilge)

[{{< fa link >}} juliasilge.com](https://juliasilge.com/)

</center>

::: footer
Slides created with [Quarto](https://quarto.org/)
:::
